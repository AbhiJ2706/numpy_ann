{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d416ee",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "This section provides an explanation of the matrix calculations for batch gradient descent in a neural network with \n",
    "- $n$ layers\n",
    "- any number of neurons per layer\n",
    "- any choice of differentiable loss function\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- $\\odot$ represents elementwise multiplication\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "   **Inputs and Layers:**\n",
    "   - Batch size: $n$\n",
    "   - Input features: $d_0$ (number of features in the input layer)\n",
    "   - $L$: Total number of layers (including the output layer)\n",
    "   - $d_l$: Number of neurons in layer $l$, for $l = 1, 2, ..., L$.\n",
    "\n",
    "   **Weights and Biases:**\n",
    "   - $\\tilde{W}_l$: Weight matrix for layer $l$, including biases.\n",
    "\n",
    "   **Activations and Pre-activations:**\n",
    "   - $Z_l$: Pre-activation (weighted sum) at layer $l$.\n",
    "   - $H_l$: Activation at layer $l$ (a#er applying activation function).\n",
    "   - $\\tilde{H}_l$: Activation at layer $l$, augmented with bias column of ones.\n",
    "\n",
    "   **Loss Function:**\n",
    "   - $\\mathcal{L}$: Loss function applied to the final layer $L$ and ground truth $Y$.\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "   **Initialization:**\n",
    "   - Augment the input matrix with a bias column:\n",
    "     $\\tilde{H}_0 = [X, 1], \\quad \\tilde{H}_0 \\in \\mathbb{R}^{n \\times (d_0 + 1)}.$\n",
    "\n",
    "   **Layer-wise Computation:**\n",
    "   For each layer $l = 1, 2, ..., L$:\n",
    "   1. Compute pre-activation:\n",
    "      $Z_l = \\tilde{H}_{l-1} \\tilde{W}_l, \\quad Z_l \\in \\mathbb{R}^{n \\times d_l}.$\n",
    "   2. Apply activation function $f_l$:\n",
    "      $H_l = f_l(Z_l), \\quad H_l \\in \\mathbb{R}^{n \\times d_l}.$\n",
    "   3. Augment $H_l$ with a bias column for subsequent layers (if $l < L$):\n",
    "   4. $\\tilde{H}_l = [H_l, 1], \\quad \\tilde{H}_l \\in \\mathbb{R}^{n \\times (d_l + 1)}.$\n",
    "\n",
    "   For the final layer $L$, the output is:\n",
    "   $H_L = f_L(Z_L), \\quad H_L \\in \\mathbb{R}^{n \\times d_L}.$\n",
    "\n",
    "**Compute Loss**\n",
    "\n",
    "   The loss function $\\mathcal{L}$ depends on the task and the outputs $H_L$:\n",
    "   - **Classification:** Cross-entropy loss, e.g., binary or categorical.\n",
    "   - **Regression:** Mean squared error or other loss functions.\n",
    "\n",
    "   Let:\n",
    "   $\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^n \\ell(H_L[i], Y[i]),$\n",
    "   where $\\ell$ is the loss for a single sample.\n",
    "   \n",
    "**Backpropagation**\n",
    "\n",
    "   **Initialization at the Output Layer:**\n",
    "   1. Compute gradient of the loss w.r.t. the output layer activation: $\\frac{\\partial \\mathcal{L}}{\\partial H_L}$ (Dimensions: $\\mathbb{R}^{n \\times d_L}$)\n",
    "   2. Backpropagate through the activation function of the output layer: $\\frac{\\partial \\mathcal{L}}{\\partial Z_L} = \\frac{\\partial \\mathcal{L}}{\\partial H_L} \\odot f_L'(Z_L)$ (Dimensions: $\\mathbb{R}^{n \\times d_L}$)\n",
    "\n",
    "   **Layer-wise Backpropagation:**\n",
    "   For each layer $l = L, L-1, ..., 1$:\n",
    "   1. Gradient w.r.t. weights: $\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{W}_l} = \\frac{1}{n} \\tilde{H}_{l-1}^\\top \\frac{\\partial \\mathcal{L}}{\\partial Z_l}$ (Dimensions: $\\mathbb{R}^{(d_{l-1} + 1) \\times d_l}$)\n",
    "   2. Backpropagate to the previous layerâ€™s activations (if $l > 1$): $\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{H}_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_l} \\tilde{W}_l^\\top$ (Dimensions:  $\\mathbb{R}^{n \\times (d_{l-1} + 1)}$)\n",
    "   3. Drop the gradient of the bias term from $\\frac{\\partial \\mathcal{L}}{\\partial \\tilde{H}_{l-1}}$: $\\frac{\\partial \\mathcal{L}}{\\partial H_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{H}_{l-1}}[:, :-1] $ (Dimensions: $\\mathbb{R}^{n \\times d_{l-1}}$)\n",
    "   4. Backpropagate through the activation function: $\\frac{\\partial \\mathcal{L}}{\\partial Z_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial H_{l-1}} \\odot f_{l-1}'(Z_{l-1})$ (Dimensions: $\\mathbb{R}^{n \\times d_{l-1}}$)\n",
    "\n",
    "Note that $H_{0}$ is the input $X$, and $\\tilde{H}_0 = [X, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a247ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, num_layers, input_features, layer_sizes, activations, activation_gradient, loss, loss_gradient, learning_rate):\n",
    "        self.num_layers = num_layers\n",
    "        self.input_features = input_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        w = [self.input_features] + layer_sizes\n",
    "        self.W = [\n",
    "            np.random.normal(0, 0.05, size=(w[i] + 1, w[i + 1])) for i in range(len(w) - 1)\n",
    "        ]\n",
    "        self.activations = activations\n",
    "        self.activation_gradient = activation_gradient\n",
    "        self.loss = loss\n",
    "        self.loss_gradient = loss_gradient\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, X: pd.DataFrame):\n",
    "        X_current = X.to_numpy()\n",
    "        X_current = np.concatenate((X_current, np.ones((X_current.shape[0], 1))), axis=1)\n",
    "        self.pre_activations = []\n",
    "        self.post_activations = [X_current]\n",
    "        for i in range(self.num_layers):\n",
    "            self.pre_activations.append(X_current @ self.W[i])\n",
    "            X_current = np.concatenate((\n",
    "                self.activations[i](self.pre_activations[-1]), \n",
    "                np.ones((self.post_activations[-1].shape[0], 1))), \n",
    "            axis=1)\n",
    "            self.post_activations.append(X_current)\n",
    "        return self.post_activations[-1][:, :-1]\n",
    "    \n",
    "    def backward(self, H: np.ndarray, y: np.ndarray):\n",
    "        dldH = self.loss_gradient(H, y)\n",
    "        dldZ = dldH * self.activation_gradient[-1](self.pre_activations[-1])\n",
    "        weight_gradients = []\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            dldW = (1 / H.shape[0]) * self.post_activations[-i - 1].T @ dldZ\n",
    "            weight_gradients.append(dldW)\n",
    "            if i < self.num_layers:\n",
    "                dldH_1 = dldZ @ self.W[-i].T\n",
    "                dldH = dldH_1[:, :-1]\n",
    "                dldZ = dldH * self.activation_gradient[-i - 1](self.pre_activations[-i - 1])\n",
    "        weight_gradients = weight_gradients[::-1]\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = self.W[i] - self.learning_rate * weight_gradients[i]\n",
    "        return self.loss(H, y)\n",
    "    \n",
    "    def __train(self, X, y):\n",
    "        H = self.forward(X)\n",
    "        return self.backward(H, y)\n",
    "\n",
    "    def fit(self, X, y, iterations=100):\n",
    "        for i in range(iterations):\n",
    "            loss = self.__train(X, y)\n",
    "            print(f\"iteration {i + 1}/{iterations}: Error\", loss)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928aba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428315b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['variety'])\n",
    "df[\"variety\"] = df[\"variety\"].apply(\n",
    "    lambda x: np.array([1, 0, 0]) if x == \"Setosa\" else np.array([0, 1, 0]) if x == \"Virginica\" else np.array([0, 0, 1])\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df[\"variety\"], test_size=0.33, random_state=42)\n",
    "del X_train[\"variety\"]\n",
    "del X_test[\"variety\"]\n",
    "\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = StandardScaler().fit_transform(X_train[col].to_numpy().reshape((-1, 1)))\n",
    "for col in X_test.columns:\n",
    "    X_test[col] = StandardScaler().fit_transform(X_test[col].to_numpy().reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArtificialNeuralNetwork(\n",
    "    num_layers=2,\n",
    "    input_features=4,\n",
    "    layer_sizes=[5, 3],\n",
    "    activations=[lambda x: 1 / (1 + np.exp(-x)) for _ in range(3)],\n",
    "    activation_derivatives=[lambda x: (1 / (1 + np.exp(-x))) * (1 - (1 / (1 + np.exp(-x)))) for _ in range(3)],\n",
    "    loss=lambda x, y: np.mean((x - y) ** 2),\n",
    "    loss_gradient=lambda x, y: 2 * (x - y),\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "model.fit(X_train, np.array(list(map(lambda x: x.tolist(), y_train.to_numpy()))), iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e98385",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.predict(X_test)\n",
    "\n",
    "y_test = np.array(list(map(lambda x: x.tolist(), y_test.to_numpy())))\n",
    "\n",
    "print(\"accuracy score:\", accuracy_score(np.argmax(test_result, axis=1), np.argmax(y_test, axis=1)))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(np.argmax(test_result, axis=1), np.argmax(y_test, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
