{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbeb615-71ed-4577-bf4a-ea3604b7d78b",
   "metadata": {},
   "source": [
    "## ANN High-Level Diagram\n",
    "\n",
    "```\n",
    "           f_1(X)      f_2(X)             f_N(X)\n",
    "          +------+    +------+           +------+  output y\n",
    "input     |      | -> |      | -> ... -> |      | ---------->        comparison\n",
    "--------> |      |    |      |           |      |                vs. expected output\n",
    "          |      | <- |      | <- ... <- |      | <----------         E(y, y')\n",
    "          +------+    +------+           +------+   error Z\n",
    "           b_1(Z)      b_2(Z)             b_N(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1a712-214b-45ce-835a-922b581d9d7e",
   "metadata": {},
   "source": [
    "## Setting up the network\n",
    "\n",
    "We need to choose a few hyperparameters:\n",
    "\n",
    "- number of layers $N$\n",
    "- input and output size of each layer $(n_1, m_1), \\dots , (n_N, m_N)$ such that $m_i = n_{i+1}, i \\in [1, N-1]$.\n",
    "- activation function for each layer\n",
    "- optimizer function (if using a library which supports it)\n",
    "- learning rate $l$\n",
    "- loss function $E$\n",
    "\n",
    "Weights are initialized randomly or set to 0 at the beginning of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccde18-49ed-4b4c-8534-2d781a8322de",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We can define a layer function $f_i(\\overrightarrow{x})$ as follows:\n",
    "\n",
    "$\\overrightarrow{y} = f_i(\\overrightarrow{x}; \\mathbf{W_i}, \\overrightarrow{b_i}; n, m, \\sigma_i) = \\sigma_i(\\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i})$\n",
    "\n",
    "$\\mathbf{W_i}$ and $\\overrightarrow{b_i}$ are trainable parameters known as weights and biases respectively. $n$, $m$, and $\\sigma_i$ are non-trainable hyperparameters referring to the input size, output size, and activation functions respectively.\n",
    "\n",
    "We define a matrix of weights $\\mathbf{W_i}$ for the layer function. The matrix looks like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{21} & ... & w_{n1}\\\\\n",
    "w_{12} & w_{22} & ... & w_{n2}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "w_{1m} & w_{2m} & ... & w_{nm}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each row of the matrix corresponds to the weights of 1 neuron. $n$ refers to the size of the input ($\\overrightarrow{x}$) and $m$ refers to the size of the output ($\\overrightarrow{y}$). The bias vector looks like this:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "b_{m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We apply the layer function in a chain: we pass the input to the first layer $f_1(\\overrightarrow{x})$. The output of this function is passed to the second layer ($f_2(\\overrightarrow{x})$. It follows that the output size for layer 1 is the input size of layer 2, the output size of layer 2 is the input size of layer 3, and so on. All in all, the output of the network after the full forward propagation process is $f_N(f_{N-1}(\\dots f_2(f_1(\\overrightarrow{x}))))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92351b-2fdb-41a8-9dde-90620981ce1f",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For backpropagation, the error metric calculated using the output of the neural network is best interpreted as a function of the weights and biases.\n",
    "\n",
    "The backpropagation function takes the error (our \"y value\"), and current weights (our \"x value\") as input, and spits out updated weights (a set of values closer to our desired \"x value\". We can't find the exact \"x value\" that makes our error 0 in just 1 step, or probably ever, given that the size of our input is so large). Recall our forward propagation function:\n",
    "\n",
    "$f_i(\\overrightarrow{x}; \\mathbf{W_i}, \\overrightarrow{b_i}; n, m, \\sigma_i) = \\sigma_i(\\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i})$\n",
    "\n",
    "We can rewrite in terms of our weights:\n",
    "\n",
    "$f_i(\\mathbf{W_i}; \\overrightarrow{b_i}; n, m, \\overrightarrow{x}, \\sigma_i) = \\sigma_i(\\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i})$\n",
    "\n",
    "### Output Layer backpropagation\n",
    "\n",
    "We want to minimize the error $E$ by shifting the weights $\\mathbf{W_i}, \\overrightarrow{b_i}$. That is, we want to know, for what change in $\\mathbf{W_i}$ and $\\overrightarrow{b_i}$ will $E$ decrease? We look at each separately. \n",
    "\n",
    "### $\\mathbf{W_N}$\n",
    "\n",
    "To get the necessary change in $\\mathbf{W_N}$, we first need the gradient of $E$ with respect to $\\mathbf{W_{N}}$, where $N$ is the last (output) layer. The gradient for our function is as follows, using the chain rule:\n",
    "\n",
    "Let $g_i(\\mathbf{W_i}) = \\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i}$. Then, $f_i = \\sigma_i(g_i(\\mathbf{W_i}))$. \n",
    "\n",
    "Recall that for our output layer, $E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_i(\\mathbf{W_i}), \\overrightarrow{a}) = E(\\sigma_i(g_i(\\mathbf{W_i})), \\overrightarrow{a})$.\n",
    "\n",
    "Now, let's assume $\\overrightarrow{a}$ is given, then we can rewrite $E$ as $E = E(\\sigma_i(g_i(\\mathbf{W_i})))$. \n",
    "\n",
    "If follows that: $\\frac{\\partial E}{\\partial \\mathbf{W_{N}}} = E'(\\sigma_i(g_i(\\mathbf{W_i}))\\sigma'_i(g_i(\\mathbf{W_i}))g'_i(\\mathbf{W_i})$.\n",
    "\n",
    "### $\\overrightarrow{b_N}$\n",
    "\n",
    "This follows the exact same process as with $\\mathbf{W_{N}}$, though we take the gradient with respect to $\\overrightarrow{b_{N}}$. \n",
    "\n",
    "Let $g_i(\\overrightarrow{b_i}) = \\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i}$. Then, $f_i = \\sigma_i(g_i(\\overrightarrow{b_i}))$. \n",
    "\n",
    "Recall that for our output layer, $E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_N(\\overrightarrow{b_N}), \\overrightarrow{a}) = E(\\sigma_N(g_N(\\overrightarrow{b_N})), \\overrightarrow{a})$.\n",
    "\n",
    "Now, let's assume $\\overrightarrow{a}$ is given, then we can rewrite $E$ as $E = E(\\sigma_N(g_N(\\overrightarrow{b_N})))$. \n",
    "\n",
    "If follows that: $\\frac{\\partial E}{\\partial \\overrightarrow{b_N}} = E'(\\sigma_i(g_N(\\overrightarrow{b_N}))\\sigma'_N(g_N(\\overrightarrow{b_N}))g'_N(\\overrightarrow{b_N})$.\n",
    "\n",
    "### Hidden layer backpropagation\n",
    "\n",
    "Remember that at its very core, a neural network is just a composition of functions $f_{N}(f_{N-1}(...(f_1(\\overrightarrow{x}))))$. So if we look at layer $N-1$, we get the following equation with respect to the error:\n",
    "\n",
    "$E(f_N(\\overrightarrow{x}), \\overrightarrow{a}) = E(f_N(f_{N-1}(\\overrightarrow{z})), \\overrightarrow{a})$ where $\\overrightarrow{z}$ is the input to layer $N-1$.\n",
    "\n",
    "### $\\mathbf{W_{N-1}}$\n",
    "\n",
    "Recall that for our output layer, $E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_{N}(\\mathbf{W_{N}}), \\overrightarrow{a}) = E(\\sigma_{N}(g_{N}(\\mathbf{W_{N}})), \\overrightarrow{a})$.\n",
    "\n",
    "We can rewrite this a bit knowing that our neural network is just a composition of functions. Note that we now write $f_{N}, g_{N}$ as a function of $\\overrightarrow{x}$ again, but we write $f_{N-1}$ as a function of $\\mathbf{W_{N-1}}$, and we can assume that $\\overrightarrow{a}$ is given.\n",
    "\n",
    "$E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_{N}(f_{N-1}(\\mathbf{W_{N-1}})), \\overrightarrow{a}) = E(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\mathbf{W_{N-1}})))), \\overrightarrow{a}) = E(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\mathbf{W_{N-1}})))))$.\n",
    "\n",
    "Now, we have related the error we got for the output layer to the weights of the first hidden layer. We can now essentially repeat the same process that we did for the output layer, chain rule and all (though the gradients are much less fun to compute).\n",
    "\n",
    "$\\frac{\\partial E}{\\partial \\mathbf{W_{N-1}}} = E'(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\mathbf{W_{N-1}})))))\\sigma'_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\mathbf{W_{N-1}}))))g'_{N}(\\sigma_{N-1}(g_{N-1}(\\mathbf{W_{N-1}})))\\sigma'_{N-1}(g_{N-1}(\\mathbf{W_{N-1}}))g'_{N-1}(\\mathbf{W_{N-1}})$.\n",
    "\n",
    "We now need to do the same for the bias vector.\n",
    "\n",
    "### $\\overrightarrow{b_{N-1}}$\n",
    "\n",
    "$E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_{N}(f_{N-1}(\\overrightarrow{b_{N-1}})), \\overrightarrow{a}) = E(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}})))), \\overrightarrow{a}) = E(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}})))))$.\n",
    "\n",
    "Now, we have related the error we got for the output layer to the biases of layer $N-1$. We can now essentially repeat the same process that we did for the output layer, chain rule and all (though the gradients are much less fun to compute).\n",
    "\n",
    "$\\frac{\\partial E}{\\partial \\overrightarrow{b_{N-1}}} = E'(\\sigma_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}})))))\\sigma'_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}}))))g'_{N}(\\sigma_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}})))\\sigma'_{N-1}(g_{N-1}(\\overrightarrow{b_{N-1}}))g'_{N-1}(\\overrightarrow{b_{N-1}})$.\n",
    "\n",
    "### Backpropagation after layer $N-1$\n",
    "\n",
    "The process works the same. We first update the weights $\\mathbf{W_i}$. For any layer $i$, we have \n",
    "\n",
    "$g_i(\\mathbf{W_i}) = \\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i}$. Then, $f_i = \\sigma_i(g_i(\\mathbf{W_i}))$. \n",
    "\n",
    "$E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_N(f_{N-1}(\\dots f_i(\\mathbf{W_i}))), \\overrightarrow{a}) = E(\\sigma_N(g_N(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i}))))), \\overrightarrow{a})$.\n",
    "\n",
    "The gradient is \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{W_i}} = $$\n",
    "\n",
    "$$E'(\\sigma_N(g_N(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i})))))$$\n",
    "\n",
    "$$\\sigma'_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i}))))))$$\n",
    "\n",
    "$$g'_{N}(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i})))))$$\n",
    "\n",
    "$$\\sigma'_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i}))))$$\n",
    "\n",
    "$$g'_{N-1}(\\dots \\sigma_i(g_i(\\mathbf{W_i})))$$\n",
    "\n",
    "$$\\dots \\sigma'_i(g_i(\\mathbf{W_i}))g'_i(\\mathbf{W_i})$$.\n",
    "\n",
    "We now update the biases $\\overrightarrow{b_i}$. For any layer $i$, we have \n",
    "\n",
    "$g_i(\\overrightarrow{b_i}) = \\mathbf{W_i}\\overrightarrow{x} + \\overrightarrow{b_i}$. Then, $f_i = \\sigma_i(g_i(\\overrightarrow{b_i}))$. \n",
    "\n",
    "$E(\\overrightarrow{e}, \\overrightarrow{a}) = E(f_N(f_{N-1}(\\dots f_i(\\overrightarrow{b_i}))), \\overrightarrow{a}) = E(\\sigma_N(g_N(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i}))))), \\overrightarrow{a})$.\n",
    "\n",
    "The gradient is \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\overrightarrow{b_i}} = $$\n",
    "\n",
    "$$E'(\\sigma_N(g_N(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i})))))$$\n",
    "\n",
    "$$\\sigma'_{N}(g_{N}(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i}))))))$$\n",
    "\n",
    "$$g'_{N}(\\sigma_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i})))))$$\n",
    "\n",
    "$$\\sigma'_{N-1}(g_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i}))))$$\n",
    "\n",
    "$$g'_{N-1}(\\dots \\sigma_i(g_i(\\overrightarrow{b_i})))$$\n",
    "\n",
    "$$\\dots \\sigma'_i(g_i(\\overrightarrow{b_i}))g'_i(\\overrightarrow{b_i})$$.\n",
    "\n",
    "### Updating the weights and biases\n",
    "\n",
    "Once we have the gradient, updating the weights and biases is very simple.\n",
    "\n",
    "$\\mathbf{W_i} = \\mathbf{W_i} - l \\cdot \\frac{\\partial E}{\\partial \\mathbf{W_i}}\\overrightarrow{1_m^T}$ (since the partial derivative is a vector)\n",
    "\n",
    "$\\overrightarrow{b_i} = \\overrightarrow{b_i} - l \\cdot \\frac{\\partial E}{\\partial \\overrightarrow{b_i}}$\n",
    "\n",
    "## Do these equations translate directly to code?\n",
    "\n",
    "Mostly. There's some nuances though. It's easy for notational purposes to write the equations the way we did but there's some extra matrix/vector transposition to make sure the dimensions line up. See the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dac7ed-a0c5-4bbf-8131-1d87c658dd4a",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ec3a3-3a7c-4908-9d51-d5e9b68363ca",
   "metadata": {},
   "source": [
    "### *Sidenote: does it work?*\n",
    "\n",
    "The answer is mostly not. The sample network is set up to solve the iris dataset- given 4 attributes of a flower it needs to classify the flower into 1 of 3 categories. It does not output the correct prediction- it outputs the probability of getting a given type of flower. I'm not sure why it does that. There's probably a typo somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff5b17-d040-475f-be85-057eecdac803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import math\n",
    "import statistics\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cf210-434e-432b-853a-69897d42253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n, m, lr, activation, derivative, loss, loss_derivative, next=None, prev=None, batch_size=32):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = derivative\n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "        self.next = next\n",
    "        self.prev = prev\n",
    "        \n",
    "        self.W = np.matrix(np.random.normal(loc=0, scale=1/n, size=n * m).reshape(m, n))\n",
    "        self.b = np.zeros(m)\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        self.inputs.append(input)\n",
    "        self.outputs.append(np.asarray(np.matmul(self.W, input) + self.b).reshape(-1))\n",
    "        if self.next:\n",
    "            return self.next.forward_propagation(self.activation(self.outputs[-1]), bn=bn)\n",
    "        else:\n",
    "            return self.activation(self.outputs[-1])\n",
    "\n",
    "    def backpropagation(self, expected_outputs=None, delta_list=None):\n",
    "        if len(self.outputs) < self.batch_size:\n",
    "            return False\n",
    "        bias_update = np.zeros(self.m).reshape(self.m,)\n",
    "        weight_update = np.zeros(self.n * self.m).reshape(self.m, self.n)\n",
    "        deltas = []\n",
    "        if not self.next:\n",
    "            if expected_outputs is None:\n",
    "                raise ValueError(\"error not given to output layer\")\n",
    "            for input, output, result in zip(self.inputs, self.outputs, expected_outputs):\n",
    "                inp = input.reshape(-1, 1)\n",
    "                loss_d = np.dot(self.loss_derivative(self.activation(output), result), self.activation_derivative(output)).reshape(-1, 1)\n",
    "                bias_update += loss_d.reshape(-1)\n",
    "                weight_update += np.dot(loss_d, inp.T)\n",
    "                deltas.append(np.asarray(np.dot(self.W.T, loss_d.reshape(-1))).reshape(-1))\n",
    "            if self.prev:\n",
    "                ret = self.prev.backpropagation(delta_list=deltas)\n",
    "            else:\n",
    "                ret = True\n",
    "        else:\n",
    "            if deltas is None:\n",
    "                raise ValueError(\"delta not given to hidden layer\")\n",
    "            for input, output, delta in zip(self.inputs, self.outputs, delta_list):\n",
    "                inp = input.reshape(-1, 1)\n",
    "                dt = delta.reshape(1, -1)\n",
    "                weight_delta = (delta * self.activation_derivative(output)).reshape(-1) # size m\n",
    "                bias_update += weight_delta\n",
    "                weight_update += np.dot(weight_delta.reshape(-1, 1), inp.T)\n",
    "                deltas.append(np.asarray(np.dot(self.W.T, weight_delta)).reshape(-1))\n",
    "            if self.prev:\n",
    "                ret = self.prev.backpropagation(delta_list=deltas)\n",
    "            else:\n",
    "                ret = True\n",
    "        \n",
    "        self.W -= self.lr / self.batch_size * weight_update\n",
    "        self.b += self.lr / self.batch_size * bias_update\n",
    "\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841b26a-e6bc-4f85-a732-e43ee75a22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=47)\n",
    "\n",
    "# building training set\n",
    "X_tr, y_tr = shuffle(iris.data, iris.target)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(y_tr)):\n",
    "    X_tr[i] -= statistics.mean(X_tr[i])\n",
    "    X_tr[i] /= statistics.stdev(X_tr[i])\n",
    "    \n",
    "    X_train.append(X_tr[i])\n",
    "    y_train.append([0.98 if y_tr[i] == j else 0.01 for j in range(3)])\n",
    "\n",
    "# activation functions\n",
    "sigmoid = lambda x: np.array([1 / (1 + math.exp(i)) for i in np.clip(x, -500, 500)])\n",
    "d_sigmoid = lambda x: sigmoid(x) * (np.ones(len(x)) - sigmoid(x))\n",
    "\n",
    "tanh = lambda x: np.array([(math.exp(i) - math.exp(-i)) / (math.exp(i) + math.exp(-i)) for i in x])\n",
    "d_tanh = lambda x: np.ones(len(x)) - tanh(x) ** 2\n",
    "\n",
    "relu = lambda x: np.array(list(map(lambda y: y if y > 0 else 0, x)))\n",
    "d_relu = lambda x: np.array(list(map(lambda y: 1 if y > 0 else 0, x)))\n",
    "\n",
    "softmax = lambda x: np.exp(np.clip(x, -500, 500)) / np.sum(np.exp(np.clip(x, -500, 500)))\n",
    "\n",
    "def d_softmax(x):\n",
    "    s = np.clip(x, -500, 500).reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "# loss functions\n",
    "mse = lambda x, y: np.array([(a - b) ** 2 for a, b in zip(x, y)])\n",
    "d_mse = lambda x, y: np.array([-2 * (a - b) for a, b in zip(y, x)])\n",
    "\n",
    "cce = lambda x, y: -np.sum([b * np.log(a + 10 ** -100) for a, b in zip(x, y)])\n",
    "d_cce = lambda x, y: np.array([-b / (a + 10 ** -100) + (1 - b) / (1 - a + 10 ** -100) for a, b in zip(x, y)])\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.5\n",
    "num_epochs = 500\n",
    "bs = 2\n",
    "\n",
    "# building network\n",
    "l1 = Layer(4, 50, lr, sigmoid, d_sigmoid, mse, d_mse, batch_size=bs)\n",
    "l2 = Layer(50, 20, lr, sigmoid, d_sigmoid, mse, d_mse, batch_size=bs)\n",
    "l3 = Layer(20, 10, lr, sigmoid, d_sigmoid, mse, d_mse, batch_size=bs)\n",
    "l4 = Layer(10, 3, lr, softmax, d_softmax, cce, d_cce, batch_size=bs)\n",
    "\n",
    "l1.next = l2\n",
    "l2.prev = l1\n",
    "l2.next = l3\n",
    "l3.prev = l2\n",
    "l3.next = l4\n",
    "l4.prev = l3\n",
    "\n",
    "nn = [l1, l2, l3, l4]\n",
    "\n",
    "# training loop\n",
    "outputs_buffer = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    losses = []\n",
    "    print(f\"Epoch: {i+1}\")\n",
    "\n",
    "    trip = False\n",
    "\n",
    "    for X, y in tqdm(zip(X_train[:10000], y_train[:10000])):\n",
    "        pred = nn[0].forward_propagation(X)\n",
    "        try:\n",
    "            losses.append(statistics.mean(nn[-1].loss(pred, y)))\n",
    "        except:\n",
    "            losses.append(nn[-1].loss(pred, y))\n",
    "            if losses[-1] > 3: trip = True\n",
    "        outputs_buffer.append(y)\n",
    "        successful = nn[-1].backpropagation(expected_outputs=outputs_buffer)\n",
    "        if successful:\n",
    "            outputs_buffer = []\n",
    "        if trip:\n",
    "            break\n",
    "    if trip:\n",
    "        break\n",
    "    print(f\"Average loss: {statistics.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d645186-b0c0-4f22-bcf2-2fbbf12bc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "for i in range(45, 50):\n",
    "    print(nn[0].forward_propagation(X_train[i]), y_train[i])\n",
    "    print(nn[-1].loss(nn[0].forward_propagation(X_train[i]), y_train[i]))\n",
    "    print(nn[-1].loss_derivative(nn[0].forward_propagation(X_train[i]), y_train[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
